import os
import re
import random
import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from tqdm import tqdm
import urllib
import json
from random import uniform
from time import sleep


def safe_navigate(driver, url):
    driver.get(url)
    sleep(uniform(1.5, 3.5))  # è®¾ç½®1.5-3.5ç§’éšæœºç­‰å¾…


class WanfangPaperCrawler:
    '''ä¸‡æ–¹æ•°æ®è®ºæ–‡çˆ¬è™«ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å…³é”®è¯ï¼‰'''

    def __init__(self):
        self.base_url = 'https://s.wanfangdata.com.cn'
        self.driver = self.init_browser()
        self.output_dir = 'output'
        os.makedirs(self.output_dir, exist_ok=True)

    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        chrome_options = Options()
        # chrome_options.add_argument("--headless")  # è°ƒè¯•æ—¶å»ºè®®å…ˆå…³é—­æ— å¤´æ¨¡å¼
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        try:
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)
            driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                "source": """
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                """
            })
            return driver
        except Exception as e:
            print(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            exit(1)

    def random_delay(self, min_sec=2.5, max_sec=5):
        """éšæœºå»¶è¿Ÿ"""
        time.sleep(random.uniform(min_sec, max_sec))
    

    def search_papers(self, keyword, max_papers=10, file_format='csv'):
        """æœç´¢è®ºæ–‡"""
        data = []
        try:
            # æ„å»ºå¸¦å¤šå¹´åº¦+æœŸåˆŠç±»å‹ç­›é€‰çš„URLï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
            base_params = {
                "q": keyword,
                "facet": [
                    {  # æ—¶é—´ç­›é€‰æ¡ä»¶
                        "PublishYear": {
                            "title": "å¹´ä»½",
                            "label": ["2022", "2023", "2024", "2025"],
                            "value": ["2022", "2023", "2024", "2025"]
                        }
                    },
                    {  # æ–°å¢æœŸåˆŠç±»å‹ç­›é€‰
                        "Type": {
                            "title": "èµ„æºç±»å‹",
                            "label": ["æœŸåˆŠè®ºæ–‡"],
                            "value": ["Periodical"]  # ä¸‡æ–¹æœŸåˆŠè®ºæ–‡çš„APIæ ‡è¯†
                        }
                    }
                ]
            }
            encoded_facet = urllib.parse.quote(json.dumps(base_params['facet']))
            search_url = f"{self.base_url}/paper?q={urllib.parse.quote(keyword)}&facet={encoded_facet}"
            
            print(f"ğŸŒ æ­£åœ¨è®¿é—®æœç´¢é¡µé¢: {search_url}")
            self.driver.get(search_url)
            self.random_delay(2.5, 5)

            # éªŒè¯ç­›é€‰æ¡ä»¶ï¼ˆæ–°å¢æ£€æµ‹ç‚¹ï¼‰
            try:
                # æ£€æµ‹å¹´ä»½ç­›é€‰
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 
                    '[data-filter-key="PublishYear"] .ivu-checkbox-wrapper-checked'))
                )
                
                # æ£€æµ‹æœŸåˆŠç±»å‹ç­›é€‰ï¼ˆæ–°å¢ï¼‰
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR,
                    '[data-filter-key="Type"] .ivu-checkbox-wrapper-checked'))
                )
                print("âœ… å¤šå¹´åº¦+æœŸåˆŠç±»å‹è¿‡æ»¤æ¡ä»¶å·²ç”Ÿæ•ˆ")
            except TimeoutException:
                print("âš ï¸ æœªæ£€æµ‹åˆ°å®Œæ•´è¿‡æ»¤æ¡ä»¶ï¼Œå¯èƒ½ç­›é€‰æœªç”Ÿæ•ˆ")

            papers_per_page = 20
            total_pages = (max_papers + papers_per_page - 1) // papers_per_page

            # ç”Ÿæˆåˆ†é¡µURLåˆ—è¡¨ï¼ˆä¿®æ­£å¾ªç¯é€»è¾‘ï¼‰
            page_urls = [f"{search_url}&p={page}" for page in range(1, total_pages + 1)]

            collected = 0
            current_page = 1
            
            while True:
                print(f"\næ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ")
                # self.driver.save_screenshot(f'page_{current_page}_debug.png')  # è°ƒè¯•ç”¨
                
                # è·å–å½“å‰é¡µè®ºæ–‡åˆ—è¡¨ï¼ˆå¢åŠ æ˜¾å¼ç­‰å¾…ï¼‰
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '.title-area'))
                )
                papers = self.driver.find_elements(By.CSS_SELECTOR, '.title-area')
                print(f"æœ¬é¡µæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")


                for paper in tqdm(papers, desc="å¤„ç†è®ºæ–‡"):
                    if collected >= max_papers:
                        break

                    try:
                        # è·å–è®ºæ–‡æ ‡é¢˜å…ƒç´ 
                        title_element = paper.find_element(By.CSS_SELECTOR, '.title')

                        # è·å–è®ºæ–‡é“¾æ¥ - é€šè¿‡ç‚¹å‡»æ ‡é¢˜è¿›å…¥è¯¦æƒ…é¡µ
                        title = title_element.text.strip()
                        print(f"\nå¤„ç†è®ºæ–‡: {title[:50]}...")

                        title_element.click()
                        self.random_delay(2, 3)

                        # åˆ‡æ¢åˆ°æ–°æ‰“å¼€çš„æ ‡ç­¾é¡µ
                        if len(self.driver.window_handles) > 1:
                            self.driver.switch_to.window(self.driver.window_handles[1])
                            current_url = self.driver.current_url

                            # è·å–è®ºæ–‡è¯¦æƒ…
                            paper_info = self.get_paper_details()
                            if paper_info:
                                data.append(paper_info)
                                collected += 1
                                print(f"[æˆåŠŸ] å·²æ”¶é›† {collected}/{max_papers} ç¯‡è®ºæ–‡")
                                self.random_delay()

                            # å…³é—­è¯¦æƒ…é¡µæ ‡ç­¾å¹¶åˆ‡æ¢å›åˆ—è¡¨é¡µ
                            self.driver.close()
                            self.driver.switch_to.window(self.driver.window_handles[0])
                        else:
                            print("æœªèƒ½æˆåŠŸæ‰“å¼€è®ºæ–‡è¯¦æƒ…é¡µ")
                            continue

                    except Exception as e:
                        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                        # å¢å¼ºçª—å£çŠ¶æ€æ£€æŸ¥ï¼ˆç½‘é¡µ7å»ºè®®ï¼‰
                        current_handles = self.driver.window_handles
                        if len(current_handles) > 1:
                            try:
                                self.driver.switch_to.window(current_handles[1])
                                self.driver.close()
                            except Exception:
                                pass
                            finally:
                                self.driver.switch_to.window(current_handles[0])
                if collected >= max_papers:
                    break

                # æ¨¡æ‹Ÿç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’® (å…³é”®ä¿®æ”¹)
                try:
                    # Step 1: å®šä½ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆæ ¹æ®ä½ æä¾›çš„DOMç»“æ„ï¼‰
                    next_btn = WebDriverWait(self.driver, 10).until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, 'span.next'))
                    )

                    # Step 2: æ»šåŠ¨åˆ°åˆ†é¡µåŒºåŸŸï¼ˆç¡®ä¿æŒ‰é’®å¯è§ï¼‰
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_btn)
                    self.random_delay(1, 1.5)  # çŸ­å»¶è¿Ÿè®©æ»šåŠ¨å®Œæˆ

                    # Step 3: ç›´æ¥ç‚¹å‡»ï¼ˆæ— éœ€ActionChainsï¼‰
                    next_btn.click()

                    # Step 4: éªŒè¯æ˜¯å¦ç¿»é¡µæˆåŠŸï¼ˆé€šè¿‡æ£€æµ‹activeé¡µç ï¼‰
                    WebDriverWait(self.driver, 15).until(
                        EC.text_to_be_present_in_element(
                            (By.CSS_SELECTOR, 'span.pager.active'),
                            str(current_page + 1)
                        )
                    )
                    current_page += 1
                    print(f"âœ… æˆåŠŸè·³è½¬åˆ°ç¬¬ {current_page} é¡µ")

                except TimeoutException:
                    print("âš ï¸ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                except Exception as e:
                    print(f"ç¿»é¡µå¤±è´¥: {str(e)}")
                    break

            # ä¿å­˜æ•°æ®
            if data:
                self.save_data(data, keyword, file_format)

        except Exception as e:
            print(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        finally:
            if data:
                self.save_data(data, keyword, file_format)

    def get_paper_details(self):
        """è·å–è®ºæ–‡è¯¦æƒ…ï¼ˆåŒ…å«å®Œæ•´å¹´ä»½ä¿¡æ¯ï¼‰"""
        try:
            # ç­‰å¾…åŒ…å«å¹´ä»½çš„å…ƒç´ åŠ è½½
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'div.itemUrl'))
            )

            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            # æ ‡é¢˜ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            title_element = soup.select_one('.detailTitleCN span')
            title = title_element.text.strip() if title_element else ''

            # å¹´ä»½ä¿¡æ¯æå–
            year = "æœªçŸ¥"
        
            # Case 1: å¤„ç†å­¦ä½è®ºæ–‡æ ¼å¼
            thesis_year_div = soup.find('div', class_='thesisYear list')
            if thesis_year_div:
                item_url = thesis_year_div.find('div', class_='itemUrl')
                if item_url:
                    year_span = item_url.find('span')
                    if year_span and year_span.text.strip().isdigit():
                        year = year_span.text.strip()

            # Case 2: å¤„ç†æœŸåˆŠè®ºæ–‡æ ¼å¼
            if year == "æœªçŸ¥":
                publish_div = soup.find('div', class_='publish list')
                if publish_div:
                    item_url_div = publish_div.find('div', class_='itemUrl')
                    if item_url_div:
                        date_text = item_url_div.text.strip()
                        date_match = re.search(r'(?:(\d{4})-\d{2}-\d{2})|(\d{4})', date_text)
                        year = date_match.group(1) or date_match.group(2) if date_match else "æœªçŸ¥"

            # ========== æ–°å¢åˆ†ç±»å·æå–é€»è¾‘ ==========
            classification = "æœªçŸ¥"
            classify_div = soup.find('div', class_='classify list')
            
            if classify_div:
                # æƒ…å†µ1ï¼šå­¦æœ¯è®ºæ–‡ç»“æ„ï¼ˆå¸¦multi-sepå±‚ï¼‰
                multi_sep_span = classify_div.find('span', class_='multi-sep')
                if multi_sep_span:
                    classification = multi_sep_span.find('span').text.strip()
                
                # æƒ…å†µ2ï¼šæœŸåˆŠç»“æ„ï¼ˆç›´æ¥åŒ…å«åˆ†ç±»å·ï¼‰
                else:
                    item_url_div = classify_div.find('div', class_='itemUrl')
                    if item_url_div:
                        classification = item_url_div.find('span').text.strip()

                # æ¸…ç†åˆ†ç±»å·æ ¼å¼ï¼ˆç§»é™¤æ‹¬å·å†…å®¹ï¼‰
                classification = re.sub(r'$.*?$', '', classification).strip()
            
            journal_info = {}
            journal_div = soup.find('div', class_='publish list')

            if journal_div:
                # æœŸåˆŠåç§°æå–ï¼ˆæ”¯æŒä¸¤ç§DOMç»“æ„ï¼‰
                journal_name = (
                    journal_div.find('a', class_='journalName') or 
                    journal_div.find('span', class_=re.compile('journal-name'))
                )
                if journal_name:
                    journal_info['æœŸåˆŠåç§°'] = re.sub(r'[\u25a0\u2588]', '', journal_name.text.strip())  # å»é™¤ç‰¹æ®Šæ–¹å—å­—ç¬¦
                
                # ISSNæå–ï¼ˆå¢å¼ºæ­£åˆ™åŒ¹é…ï¼‰
                issn_span = journal_div.find(lambda tag: tag.name == 'span' and 'ISSN' in tag.text)
                if issn_span:
                    issn_text = re.sub(r'[^0-9X-]', '', issn_span.text.split('ï¼š')[-1])
                    journal_info['ISSN'] = issn_text[:9] if len(issn_text) > 9 else issn_text  # æ ‡å‡†åŒ–é•¿åº¦

            # # åˆå¹¶åˆ°è®ºæ–‡ä¿¡æ¯
            # paper_info.update({
            #     **journal_info,
            #     'æ–‡çŒ®ç±»å‹': 'æœŸåˆŠè®ºæ–‡'  # æ˜ç¡®æ ‡æ³¨æ–‡çŒ®ç±»å‹
            # })

            # ========== ä½œè€…å•ä½æå–é€»è¾‘ ==========
            organizations = []

            # æ ¸å¿ƒé€‰æ‹©å™¨è·¯å¾„ï¼ˆé€‚é…åŠ¨æ€ç±»åï¼‰
            org_elements = soup.select('div.organization.detailOrganization a[class*="test-detail-org"]')

            # å¤‡ç”¨é€‰æ‹©å™¨ï¼ˆå¤„ç†å¯èƒ½çš„ç±»åå˜ä½“ï¼‰
            if not org_elements:
                org_elements = soup.select('div[class*="detailOrganization"] span.multi-sep a')

            # æ–‡æœ¬æ¸…æ´—é€»è¾‘
            for org in org_elements:
                # åˆ†å‰²å¤åˆå•ä½ï¼ˆç¤ºä¾‹ï¼š"åŒ—äº¬é‚®ç”µå¤§å­¦ï¼›ç½‘ç»œç©ºé—´å®‰å…¨å­¦é™¢"ï¼‰
                units = [u.strip() for u in org.text.split('ï¼›')]
                # ç§»é™¤åœ°å€ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼š"åŒ—äº¬100876"ï¼‰
                clean_units = [re.sub(r'ï¼Œ\s*\S+$', '', u) for u in units]
                organizations.extend(clean_units)

            # å»é‡å¤„ç†
            organizations = list(set(organizations))

            # ä½œè€…ã€æ‘˜è¦ã€å…³é”®è¯ä¿æŒåŸæœ‰é€»è¾‘
            authors = []
            author_elements = soup.select('.author.detailTitle .test-detail-author span')
            for author in author_elements:
                author_text = author.get_text(strip=True)
                if author_text:
                    authors.append(author_text)

            abstract_element = soup.select_one('.summary.list .text-overflow span')
            abstract = abstract_element.text.strip() if abstract_element else ''

            keywords = []
            keyword_elements = soup.select('.keyword.list .multi-sep span')
            for word in keyword_elements:
                keywords.append(word.text.strip())

            paper_info = {
                'æ ‡é¢˜': title,
                'è®ºæ–‡å‘è¡¨å¹´ä»½': year,
                **{k: v for k, v in journal_info.items() if v},  # åŠ¨æ€åˆå¹¶æœ‰æ•ˆæœŸåˆŠå­—æ®µ
                'åˆ†ç±»å·': classification,
                'ä½œè€…': ', '.join(authors) or 'æœªçŸ¥',
                'å•ä½': '; '.join(organizations) if organizations else 'æœªçŸ¥',
                'æ‘˜è¦': abstract,
                'å…³é”®è¯': '; '.join(keywords)
            }

            print(f"æˆåŠŸè·å–è®ºæ–‡ä¿¡æ¯ | å‡ºç‰ˆå¹´ä»½: {year}")
            return paper_info

        except Exception as e:
            print(f"è·å–è®ºæ–‡è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")
            return None

    def save_data(self, data, keyword, file_format='csv'):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not data:
            print("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"wanfang_papers_{keyword}_{timestamp}"

        if file_format.lower() == 'tsv':
            filepath = os.path.join(self.output_dir, f"{filename}.tsv")
            df.to_csv(filepath, sep='\t', index=False, encoding='utf-8')
            print(f"æ•°æ®å·²ä¿å­˜ä¸ºTSVæ–‡ä»¶: {filepath}")
        else:
            filepath = os.path.join(self.output_dir, f"{filename}.csv")
            df.to_csv(filepath, index=False, encoding='utf_8_sig')
            print(f"æ•°æ®å·²ä¿å­˜ä¸ºCSVæ–‡ä»¶: {filepath}")

    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            print("\n=== ä¸‡æ–¹æ•°æ®è®ºæ–‡çˆ¬è™« ===")
            keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
            if not keyword:
                print("å¿…é¡»è¾“å…¥æœç´¢å…³é”®è¯!")
                return

            max_papers = input("è¯·è¾“å…¥è¦çˆ¬å–çš„æœ€å¤§è®ºæ–‡æ•°(é»˜è®¤10): ").strip()
            max_papers = int(max_papers) if max_papers.isdigit() else 10

            file_format = input("è¯·é€‰æ‹©ä¿å­˜æ ¼å¼ (1)CSV (2)TSV (é»˜è®¤CSV): ").strip()
            file_format = 'tsv' if file_format == '2' else 'csv'

            print(f"\nå¼€å§‹çˆ¬å–è®ºæ–‡ï¼Œå…³é”®è¯: {keyword}")
            self.search_papers(keyword, max_papers, file_format)

        except Exception as e:
            print(f"\nè¿è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        finally:
            self.driver.quit()
            print("\nç¨‹åºç»“æŸ")


if __name__ == '__main__':
    crawler = WanfangPaperCrawler()
    crawler.run()